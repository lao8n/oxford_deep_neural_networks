{"cells":[{"cell_type":"markdown","metadata":{"id":"133m8ys9nzHj"},"source":["In the previous practical we worked on a relatively small CNN-based model and observe how different approaches to regularize such model contributed to reach higher accuracies and less overfitting.\n","\n","In this practical we'll use those techinques but start by defining a more advance model based on the [ResNet](https://arxiv.org/pdf/1512.03385.pdf) architecture."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVM0Ee_aB23z"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import time\n","import os \n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","%matplotlib inline\n","\n","from IPython.display import Image"]},{"cell_type":"markdown","metadata":{"id":"Go_hM-TTEWud"},"source":["# Advanced PyTorch\n","\n","Up until now, we have mostly been using PyTorch layers (e.g. `nn.conv2d`, `nn.Sequential`, `nn.dropout`, etc) that are natively supported by the framework. The vast majority of architectures for various applications will make use of these.\n","\n","As you probably have noticed, when making use of a PyTorch layer (e.g. a convolutional layer) we only need to (1) define it as an element in the model and (2) making use of it in the `forward()` method of the model. By now we know that training involves two steps: forward and backward passes. ****But we never have to type any code to do the backpropagation of each of the layers that comprises our model!**** This is because PyTorch automatically builds a computational graph recording the order at which the operations in the forward pass takes place and follows the reverse order to perform the backpropagation. In addition, all common layers (e.g. a convolutional layer) come with built-in rules on how to perform the backward pass (i.e. you don’t have to implement it). \n","\n","However, it could happen that you want a particular feature for which PyTorch doesn’t offer an implementation. In this section we will show how to build a custom PyTorch `nn.Module` from scratch. We will be creating a quantization layer which will be useful to reduce the (1) model size of the model, (2) make use of reduced-precission arithmetic and (3) act as a regularization mechanism."]},{"cell_type":"markdown","metadata":{"id":"PTk_lgRjSWWr"},"source":["## Custom `Autograd` modules\n","Autograd makes the backpropagation (and therefore the diferentiation of all functions applied in the forward pass) effortless to the engineer/developer training a nerual network using PyTorch. From the documentation: *torch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare Tensors for which gradients should be computed with the `requires_grad=True` keyword.*\n","You can read more about it in the documentation [torch.autograd](https://pytorch.org/docs/stable/autograd.html)\n","\n","\n","\n","**Quantization as an example of an autograd module**\n","\n","As briefly introduced above, we are going to implement a quantization layer making use of custom `autograd` function. This function (see cell bellow) takes as input a standard PyTorch tensor (which by default uses 32-bit precission -- a `float`) and transforms its contents by quantizing its content to a user-defined number of bits. \n","\n","A few common approaches to perform quantization are described in [R. Krishnamoorhi (2018)](). Here, we followed symmetric per-layer quantization. Mathematically, this is implemented in the forward pass:\n","\n","`x_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale`\n","\n","During back-propagation we apply standard [Straight Through Estimator](https://arxiv.org/pdf/1308.3432.pdf). This is the standard procedure to train quantization-aware networks (often called *fake quantization* becuase we even though we quantize the data to the desired number of bits, the data type of the tensor is left very often unchanged as a `torch.float`)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CT1UXCSSEcl7"},"outputs":[],"source":["class QuantizeSymmetric(torch.autograd.Function):\n","\n","    @staticmethod\n","    def forward(ctx, input, num_bits=8, min_value=None, max_value=None):\n","\n","        output = input.clone()\n","\n","        # computing the max/min values (centered around zero) that can be covered by using `num_bits` bits\n","        qmin = -1.0 * (2**num_bits)/2\n","        qmax = -qmin - 1\n","\n","        # compute qparams --> scale and zero_point\n","        max_val, min_val = float(max_value), float(min_value)\n","        min_val = min(0.0, min_val)\n","        max_val = max(0.0, max_val)\n","\n","        # Computing scale\n","        if max_val == min_val:\n","            scale = 1.0\n","        else:\n","            max_range = max(-min_val, max_val) # largest mag(value)\n","            scale = max_range / ((qmax - qmin) / 2)\n","            scale = max(scale, 1e-8) # for stability purposes\n","\n","        # Zero_point\n","        zero_point = 0.0\n","\n","        # Quantization (implementing the equation above)\n","        output.div_(scale).add_(zero_point)\n","        output.round_().clamp_(qmin, qmax)  # quantize\n","        output.add_(-zero_point).mul_(scale)  # dequantize\n","\n","        return output\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        # straight-through estimator (STE) --> it lets the gradients form the next layer in the network to be passed straight to the previous one\n","        grad_input = grad_output\n","        return grad_input, None, None, None"]},{"cell_type":"markdown","metadata":{"id":"lMF87_sRSc5v"},"source":["## A Custom Layer\n","\n","The code cell above only defines a function with custom `forward()` and `backward()` implementations. Normally, we'd like to make use of `autograd` functions inside layers of a neural network.\n","\n","Below, we define a quantization layer. If you look closely, you'll see it contains the same structure as used when defining a network architecture: (1) we define the elements of the layer in `__init()__` and (2) we state in `forward()` what happens when an input is fed to this quantization layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mo7G_L9gSRNB"},"outputs":[],"source":["class Quant(nn.Module):\n","\n","    def __init__(self, num_bits=8, momentum=0.01):\n","        super(Quant, self).__init__()\n","        self.min_val = self.max_val = None\n","        self.momentum = momentum\n","        self.num_bits = num_bits\n","\n","    def forward(self, input):\n","        if self.training:\n","\n","            min_val = self.min_val\n","            max_val = self.max_val\n","\n","            if min_val is None or max_val is None: \n","                # First step executing quantization\n","                min_val = input.detach().min() \n","                max_val = input.detach().max() \n","            else:\n","                # equivalent to --> min_val = min_val(1-self.momentum) + self.momentum * torch.min(input)\n","                min_val = min_val + self.momentum * (input.detach().min()  - min_val)\n","                max_val = max_val + self.momentum * (input.detach().max()  - max_val)\n","\n","            self.min_val = min_val\n","            self.max_val = max_val\n","\n","        return QuantizeSymmetric().apply(input, self.num_bits, self.min_val, self.max_val)"]},{"cell_type":"markdown","metadata":{"id":"m7HyScvtSoug"},"source":["Let's create a simple example of how to generate an instance of a quantization layer that will quantize the input tensor (that by default is of type `torch.float`) into 8-bits. Then, we will visualize both, input and output tensors as well as the absolute difference (which could be seen as a metric of error introduced in the quantization process).\n","\n","Try to change the `num_bits` to see what happens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MjabcIk9SgXW"},"outputs":[],"source":["input = torch.randn(32,32)\n","quantizer = Quant(num_bits=8)\n","\n","input_q = quantizer(input)\n","\n","\n","def showTensor(tensor, ax, label):\n","  im = ax.imshow(tensor)\n","  ax.set_title(label)\n","  divider = make_axes_locatable(ax)\n","  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n","  fig.colorbar(im, cax=cax)\n","\n","\n","fig, axs = plt.subplots(1,3, figsize=(18, 9))\n","\n","showTensor(input, axs[0], \"Original\")\n","showTensor(input_q, axs[1], \"Quantized\")\n","showTensor(abs(input-input_q), axs[2], \"Absolute Diff\")"]},{"cell_type":"markdown","metadata":{"id":"wKHxFxtIcMlh"},"source":["Very recently, PyTorch realised a new version of the framework that nativelly supports multiple types of quantization (e.g. post-training quantization, training-aware quantization, etc). You can check all the details in [the documentation](https://pytorch.org/docs/stable/quantization.html)."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}