{"cells":[{"cell_type":"markdown","metadata":{"id":"hWa5v1YXL2vA"},"source":["In this practical we are going to see how to:\n","\n","*   Create a custom PyTorch dataset.\n","*   Show how data-augmentation can be applied to audio clips.\n","*   Introduce two types of audio features that can be used for classification applications.\n","\n","To put this ideas together in a self-contained project, we will show how to build a Keyword-Spotting (KWS) application. A KWS system aims to detect a particular keyword (often just one word) from a continuous audio stream. An example we're all familiar with is \"hey Siri\" or \"Alexa\". These devices often perform this detection on-device (i.e. the data is processed localy instead of in the Cloud). In this practical we are going to build a simpler KWS system that classifies 1-second audio clips by using audio-specfic features."]},{"cell_type":"markdown","metadata":{"id":"Rmpay3pPRQIQ"},"source":["We will use the popular [librosa](https://librosa.github.io) python package to perform the loading, visualization and feature extraction for our dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jizlD8qaFwz7"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","\n","import IPython.display as ipd\n","import librosa\n","import librosa.display\n","import cv2\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"yDnFJSudJpeD"},"source":["To train our KWS classifier we'll use the Google Speech Commands dataset. This dataset is comprised of 65K 1-second long audio clips (`.wav` files at 16KHz) containing a single word spoken (e.g. \"yes\", \"no\", \"up\", \"down\", etc). There are a total of 30 short words. You can read more about the dataset in [this blogpost](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) or in [this paper](https://arxiv.org/pdf/1804.03209.pdf).\n","\n","Below we download the dataset and uncompress it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s49ji5b3F4SF"},"outputs":[],"source":["# Get Google Speech Commands dataset (~1.5GB)\n","!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -P ./datasets/\n","\n","# uncompress dataset\n","!mkdir ./datasets/speech_commands_v0.01 \n","!tar -xzf ./datasets/speech_commands_v0.01.tar.gz -C ./datasets/speech_commands_v0.01"]},{"cell_type":"markdown","metadata":{"id":"Paq9_i_ZFw0D"},"source":["## Data exploration"]},{"cell_type":"markdown","metadata":{"id":"jcmck_NYNibq"},"source":["Let's start by ispecting the dataset. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GqAB05hFw0I"},"outputs":[],"source":["def inspectAudio(filename, augment=None, ratio=0.15):\n","    \n","    plt.figure(figsize=(18, 5))    \n","    \n","    # load audio clip\n","    y, sr = librosa.load(filename)\n","    \n","    if augment is not None:\n","        # load background audio file\n","        y_bkg, s_bkg = librosa.load(augment)\n","        \n","        # select a random point in y_bkg to add to audio file y\n","        num_samples = len(y)\n","        start = np.random.randint(0, len(y_bkg)-num_samples)\n","        \n","        # add y_bkg to y following ratio\n","        y_corrupted = (1.0-ratio)*y + ratio*y_bkg[start:start+num_samples]\n","    \n","    # compute spectrogram\n","    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n","\n","    # Visualization\n","    plt.subplot(1, 2, 1)\n","    librosa.display.waveplot(y) # plot waveform\n","    plt.title('Audio Waveform')\n","    \n","    plt.subplot(1, 2, 2)\n","    librosa.display.specshow(D, y_axis='log') # plot spectrogram in log scale\n","    plt.colorbar(format='%+2.0f dB')\n","    plt.title('Log-frequency power spectrogram')\n","    plt.xlabel('Time')\n","    plt.show()\n","    \n","    # audio player\n","    ipd.display(ipd.Audio(y, rate=sr))\n","    \n","    if augment:\n","#         plt.figure()\n","        plt.figure(figsize=(18, 5))    \n","        plt.subplot(1, 2, 1)\n","        librosa.display.waveplot(y_corrupted) # plot waveform\n","        plt.title('Audio Waveform + background')\n","\n","        # compute spectrogram\n","        D_corrupted = librosa.amplitude_to_db(np.abs(librosa.stft(y_corrupted)), ref=np.max)\n","    \n","        plt.subplot(1, 2, 2)\n","        librosa.display.specshow(D_corrupted, y_axis='log') # plot spectrogram in log scale\n","        plt.colorbar(format='%+2.0f dB')\n","        plt.title('Log-frequency power spectrogram')\n","        plt.xlabel('Time')\n","        plt.show()\n","        \n","        # audio player\n","        ipd.display(ipd.Audio(y_corrupted, rate=sr))\n","        \n","    \n","    return D\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RDbrwwOAFw0K"},"outputs":[],"source":["# one example in the dataset of a person saying the word \"yes\"\n","filename = \"./datasets/speech_commands_v0.01/yes/1c3f4fac_nohash_1.wav\"\n","# one example of background noise from the dataset\n","bkg = \"./datasets/speech_commands_v0.01/_background_noise_/running_tap.wav\"\n","\n","# let's visualize the audio clip and see how augmenting it by using a 0.85-0.15 clip/noise ratio.\n","spec = inspectAudio(filename, bkg, ratio=0.15)"]},{"cell_type":"markdown","metadata":{"id":"nlcuQoPYFw0O"},"source":["## Extracting audio features"]},{"cell_type":"markdown","metadata":{"id":"CXljjCVeFw0O"},"source":["### 1. Using spectrograms as features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qygchM1RFw0P"},"outputs":[],"source":["# the previously computed spectrogram isn't squared. This is not a necessary requisite for training CNNs\n","# but having such a high dimensions (1025,44) could make our training slow.\n","print(spec.shape)\n","\n","# Let's then resize our spectrogram by treating it like an image\n","spec_ = cv2.resize(np.flipud(spec), dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\n","print(spec_.shape)\n","plt.figure(figsize=(6, 6))\n","plt.imshow(spec_, cmap='inferno')"]},{"cell_type":"markdown","metadata":{"id":"8HV3KXvsFw0S"},"source":["Now that we know how to (1) load 1-second .wav files, (2) build the spectrogram of each clip and (3) reshape them so could be treated as an image of manageable dimensions, there's nothing stopping us from building a dataset by following steps (1)-(3) with all images in the Google Speech Commands dataset. We'll do that shortly."]},{"cell_type":"markdown","metadata":{"id":"fDQVT95KFw0S"},"source":["### 2. Using MFCC features\n","\n","Using the spectrogram as a first order descriptor for our one-second audio clips is a valid strategy and we'll later see how that performs. However, there's it's commonly accepted that [Mel-frequency cepstrum](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) features (MFCC) are a more compact, high-quality descriptor than using raw spectrograms.\n","\n","Extracting MFCC features is supported in several frameworks, including Librosa and TorchAudio. The cell below shows how to extract and visualize MFCC features using Librosa."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0uw0NCxFw0T"},"outputs":[],"source":["# load audio file\n","y, sr = librosa.load(filename)\n","\n","# extract 40 MFCC features\n","mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n","print('MFCC features shape:', mfccs.shape)\n","\n","# Visualization\n","plt.figure(figsize=(6, 6))\n","librosa.display.specshow(mfccs, x_axis='time', cmap='inferno')\n","plt.colorbar()\n","plt.title('MFCC')"]},{"cell_type":"markdown","metadata":{"id":"RjW6ulX0Fw0V"},"source":["Unlike with the spectrogram, we don't really need to resize the MFCC array since it's already of a quite manageable size. We'll use the MFCC features of each audio clip as inputs to the CNN classifier for our Keyword Spotting System."]},{"cell_type":"markdown","metadata":{"id":"uEdebG2IFw0V"},"source":["## Building a PyTorch dataset for KWS applications\n","\n","In order to train our KWS classifier efficiently we need to build our Google Speech Commands dataset as a Pytorch Dataset object (i.e. [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)). Let's do that now by loading first a subset (you can try all if you want) the `.wav` files."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojNtyfnoFw0W"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","from tqdm.notebook import tqdm\n","\n","# To make things simpler we'll build our dataset for this practical as a subset of the Google Speech \n","# Commands dataset. In this way, we'll only consider keywords of a few words (see below).\n","\n","keywords = [\"on\", \"off\", \"left\", \"right\", \"go\", \"stop\", \"yes\", \"no\"]\n","path = \"./datasets/speech_commands_v0.01\" # path to where the dataset folder lives\n","\n","\n","def loadDataset(path):\n","    \"\"\" Get the filenames of .wav that belongs to the chosen keywords\"\"\"\n","    labels = []\n","    audios = []\n","\n","    for i, kword in enumerate(keywords):\n","        pathKword = os.path.join(path, kword)\n","        for file in os.listdir(pathKword): # for all files in directory\n","            if file.endswith(\".wav\"): # if it's a .wav file\n","                filename = os.path.join(pathKword, file)\n","                audios.append(filename)\n","                labels.append(i)\n","\n","    return labels, audios\n","        "]},{"cell_type":"markdown","metadata":{"id":"mL94tBBzFw0W"},"source":["The cell below shows how to create a custom PyTorch Dataset class. Upon initialization, we'll be loading the entire set of `.wav` files. Then, each time we ask for a element in our dataset, the `__getitem__()` method will be executed. There, we extract the MFCC features using TorchAudio. We use 40ms windows with a 20ms overlap (as was described in the [Hello Edge](https://arxiv.org/abs/1711.07128) seminal paper)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3mxsBq69Fw0X"},"outputs":[],"source":["class keywordsDataset(Dataset):\n","\n","    def __init__(self, root_dir, useMFCC=True, transform=None):\n","\n","        # get labels - fileNames pairs\n","        self.labels, self.files = loadDataset(root_dir) \n","        self.root_dir = root_dir\n","        self.useMFCC = useMFCC\n","\n","        self.loadAudios()\n","\n","    def __len__(self):\n","        return len(self.audios)\n","\n","    def loadAudios(self):\n","        \"\"\"Here we pre-load the audio files so reading from disk doesn't impact on training speed.\"\"\"\n","        self.audios = []\n","        with tqdm(total=len(self.files), desc='Reading .wav Files') as t:\n","            for i , filename in enumerate(self.files):\n","                waveform, _ = librosa.load(filename) # load audio file\n","                self.audios.append(waveform)\n","                t.update(1)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"We reach this point everytime the dataloader asks for a new batch. Here we extract the desired audio features\n","        from the pre-loaded audio clips.\"\"\"\n","        \n","        waveform = self.audios[idx]\n","        n_fft = int(len(waveform)*0.04) # 40ms window\n","        hop_length = int(n_fft / 2) # 20ms stride\n","        \n","        # extract features\n","        if self.useMFCC:\n","            features = librosa.feature.mfcc(y=waveform, n_mfcc=40, n_fft=n_fft, hop_length=hop_length)\n","        \n","        features = torch.unsqueeze(torch.from_numpy(features), 0)\n","        \n","        # return features and label\n","        return features, self.labels[idx]"]},{"cell_type":"markdown","metadata":{"id":"b4a5eWweFw0Z"},"source":["Let's now construct the dataset and leave 10% of the audio clips for test. While this happens consider reading the paper that introduced the [Speech Commands dataset](https://arxiv.org/pdf/1804.03209.pdf) or the [Hello Edge](https://arxiv.org/pdf/1711.07128.pdf), showing how a KWS system can be implemented in micro-controllers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tAHzQpqJFw0Z"},"outputs":[],"source":["# create dataset (this might take ~10min depending on your hard drive speed)\n","myDataset = keywordsDataset(path)\n","\n","# Use 10% images for validation\n","val_size = 0.1\n","num_train = len(myDataset)\n","split = int(np.floor(val_size * num_train))\n","\n","# randomly split `myDataset` for train/val\n","train_dataset, val_dataset = torch.utils.data.random_split(myDataset,[num_train - split,split])"]},{"cell_type":"markdown","metadata":{"id":"5AUyz3PUFw0c"},"source":["Here we verify that everything works."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o06zUwTaFw0d"},"outputs":[],"source":["# get a single element from the train set\n","dataPoint = train_dataset[12]\n","print(dataPoint[0].shape) # shape of the MFCC features\n","print(dataPoint) # returns a tuple as (MFCC, label)"]},{"cell_type":"markdown","metadata":{"id":"UnaVGeDmFw0e"},"source":["Now it's time to define the [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) objects that will be feeding our network during training/evaluation. Here is where we define batch_size and the type of data augmentation to use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oHlQcF9rFw0e"},"outputs":[],"source":["train_DataLoader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8 if torch.cuda.is_available() else 0)\n","val_DataLoader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=8 if torch.cuda.is_available() else 0)"]},{"cell_type":"markdown","metadata":{"id":"GutU_cQlFw0f"},"source":["## Training a CNN for KWS"]},{"cell_type":"markdown","metadata":{"id":"NlcX7qD9Fw0f"},"source":["Now that our dataset is ready, the following cells of code should look very familiar.\n","\n","Below we define a simple CNN."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwSah1C4Fw0f"},"outputs":[],"source":["class basicCNN(nn.Module):\n","    def __init__(self, numClasses):\n","        super(basicCNN, self).__init__()\n","        self.name = \"basicCNN\"\n","        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=2),\n","                                    nn.BatchNorm2d(32))\n","        self.layer2 = nn.Sequential(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2),\n","                                    nn.BatchNorm2d(64))\n","        self.layer3 = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n","                                    nn.BatchNorm2d(128))\n","        self.relu = torch.nn.ReLU()\n","        self.pool = torch.nn.MaxPool2d(2)\n","        self.fc = nn.Linear(128 * 2*  3, numClasses)\n","        self.criterion = nn.CrossEntropyLoss()\n","\n","    def forward(self, x):\n","        x = self.relu(self.layer1(x))\n","        x = self.pool(self.relu(self.layer2(x)))\n","        x = self.relu(self.layer3(x))\n","        x = self.fc(x.view(-1, 128 * 2 * 3))\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"-Da7uewrFw0g"},"source":["Below we define the training and testing loops as well as other helper functions (everything is borrowed from the `utils.py` file we were using in the previous practicals)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQ8bIpJQFw0h"},"outputs":[],"source":["def getAccuracy(outputs, labels, num):\n","    \"\"\" Computes the accuracy\"\"\"\n","    _, predicted = torch.max(outputs, 1)\n","    correct = (predicted == labels).sum()\n","    return correct.item()/float(num)\n","\n","class RunningAverage():\n","    \"\"\"A simple class that maintains the running average of a quantity \"\"\"\n","    def __init__(self):\n","        self.steps = 0\n","        self.total = 0\n","    \n","    def update(self, val):\n","        self.total += val\n","        self.steps += 1\n","    \n","    def __call__(self):\n","        return self.total/float(self.steps)\n","    \n","def train(model, device, train_loader, writer, optimiser, epoch):\n","    model.train() \n","        \n","    loss_avg = RunningAverage()\n","    acc_avg = RunningAverage()\n","    # for every mini-batch containing batch_size images...\n","    with tqdm(total=len(train_loader.dataset), desc='Train Epoch #' + str(epoch)) as t:\n","        for i , (data, target) in enumerate(train_loader):\n","\n","            # print(data.shape)\n","            # send the data (images, labels) to the device (either CPU or GPU)\n","            inputs, labels = data.to(device), target.to(device)\n","\n","            # zero gradients from previous step\n","            optimiser.zero_grad()\n","\n","            # this executes the forward() method in the model\n","            outputs = model(inputs)\n","\n","            # compute loss\n","            loss = model.criterion(outputs, labels)\n","\n","            # backward pass\n","            loss.backward()\n","\n","            # evaluate trainable parameters\n","            optimiser.step()\n","\n","            # Monitoring progress, accuracy and loss\n","            acc_avg.update(getAccuracy(outputs, labels, inputs.shape[0]))\n","            loss_avg.update(loss.item())\n","            t.set_postfix({'avgAcc':'{:05.3f}'.format(acc_avg()), 'avgLoss':'{:05.3f}'.format(loss_avg())})\n","            t.update(data.shape[0])\n","            \n","def test(model, device, test_loader):\n","    \n","    model.eval() # no update of trainable parameters (e.g. batch norm)\n","    \n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        \n","        # now we evaluate every test image and compute the predicted labels\n","        for data in test_loader:\n","            \n","            # send data to device\n","            images, labels = data[0].to(device), data[1].to(device)\n","            \n","            # pass the images through the network\n","            outputs = model(images)\n","\n","            # obtain predicted labels\n","            _, predicted = torch.max(outputs.data, 1)\n","            \n","            # count total images in batch\n","            total += labels.size(0)\n","            # count number of correct images\n","            correct += (predicted == labels).sum()\n","    \n","    # compute accuracy\n","    test_acc = correct.item()/float(total)\n","    \n","    print(\"Accuracy on Test Set: %.4f\" % test_acc)"]},{"cell_type":"markdown","metadata":{"id":"z0vHsXrVFw0i"},"source":["Below is a simplified version of the `main()` function we used in the previous practicals."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIJRfoMxFw0j"},"outputs":[],"source":["def main(net, numEpoch, trainLoader, valLoader, use_cuda=False, lr=0.1):\n","        \n","    # 1. Define optimiser\n","    optim = torch.optim.SGD(net.parameters(), lr=lr)\n","    \n","    # 2. Define define where training/testing will take place\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    print(\"Launching training on: %s\" % device)\n","    \n","    # 2.1 Send model to device\n","    net = net.to(device)\n","    \n","    # 3. Train for `numEpoch` epochs\n","    for epoch in range(1, numEpoch + 1):\n","        train(net, device, trainLoader, None, optim, epoch)\n","              \n","    test(net, device, valLoader)\n","    "]},{"cell_type":"markdown","metadata":{"id":"NF0myHPzFw0k"},"source":["Let's train our KWS system!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SrHubiSFw0k"},"outputs":[],"source":["main(basicCNN(len(keywords)), 5, train_DataLoader, val_DataLoader, use_cuda=True)"]},{"cell_type":"markdown","metadata":{"id":"FQLZeBj9NL-1"},"source":["## (Optional) Building a custom dataset with data augmentation\n","\n","We have seen how to create a dataset using PyTorch's API. This was done by first loading all audio clips in memory and then extracting MFCC fetures everytime we need to feed another batch to the CNN (i.e. each time we need to perform a new training step).\n","\n","However, it could happen that the dataset is so big, that it cannnot possibly fit in memory. When this is the case, we'd normally load the files during batch creation (i.e. in `__getitem__`).\n","\n","**Exercise**\n","\n","\n","\n","*   Create a new PyTorch dataset that loads the files each time a new batch needs to be generated (i.e. in `__getitem__`)\n","*   After loading it, apply some data augmentation, for instance by superpossing another audio clip with background noises (as we saw at the top of this notebook)\n","*   Then extract the MFCC features.\n","*   To verify you've done it correctly perform a 5 epoch training.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrPiYtWuFw0l"},"outputs":[],"source":["class keywordsDatasetv2(Dataset):\n","\n","    def __init__(self, root_dir, useMFCC=True, corrupt=0.10):\n","\n","        # get labels - fileNames pairs\n","        self.labels, self.files = loadDataset(root_dir) \n","        self.root_dir = root_dir\n","        self.useMFCC = useMFCC\n","\n","        # load background audio file (you could randomize this as well)\n","        self.y_bkg, _ = librosa.load(\"./datasets/speech_commands_v0.01/_background_noise_/running_tap.wav\")\n","        self.corrupt = corrupt\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"We reach this point everytime the dataloader asks for a new batch. Here we (1) load the audio clips, \n","        (2) apply data augmentation and (3) extract the desired audio features.\"\"\"\n","        \n","        return None, None"]},{"cell_type":"markdown","metadata":{"id":"y62PEYkvPU86"},"source":["Now let's create an intance of our new dataset and the associated dataloaders. We'll also follow the same strategy as before to perform the train/test split. Then train this model using the dataset you just created.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgX2kdQVO6DO"},"outputs":[],"source":["# create dataset\n","# myDataset = keywordsDatasetv2(path, useMFCC=False, corrupt=0.0) # this will generate a dataset using Spectrogram as features and no augmentation\n","myDataset = keywordsDatasetv2(path, useMFCC=True, corrupt=0.10) # this will generate a dataset using MFCC as features and adding 10% of background noise\n","\n","# Use 10% images for validation\n","val_size = 0.1\n","num_train = len(myDataset)\n","split = int(np.floor(val_size * num_train))\n","\n","# randomly split `myDataset` for train/val\n","train_dataset, val_dataset = torch.utils.data.random_split(myDataset,[num_train - split,split])"]},{"cell_type":"markdown","metadata":{"id":"Tk7sJV1miLWc"},"source":["Now we create the dataloader objects that will consume the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jv5uH5ZvgzLe"},"outputs":[],"source":["train_DataLoader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2 if torch.cuda.is_available() else 0)\n","val_DataLoader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2 if torch.cuda.is_available() else 0)"]},{"cell_type":"markdown","metadata":{"id":"3-rqA8ItPs4-"},"source":["Now let's train for a few epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67mIFiZaPsX0"},"outputs":[],"source":["main(basicCNN(len(keywords)), 5, train_DataLoader, val_DataLoader, use_cuda=True)"]},{"cell_type":"markdown","metadata":{"id":"4mC8jDbJWihX"},"source":["**Observations**\n","\n","\n","\n","*   Is it faster training without a pre-loaded dataset?\n","*   How do you think this could be solved?\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.7"}},"nbformat":4,"nbformat_minor":0}